---
title: "Visual MCMC diagnostics using the bayesplot package"
author: "Jonah Gabry"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 4
vignette: >
  %\VignetteIndexEntry{Visual MCMC diagnostics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, settings, include=FALSE}
library("bayesplot")
knitr::opts_chunk$set(
  dev = "svg",
  fig.align = "center",
  fig.width = 6,
  fig.height = 4,
  comment = NA
)
```

This vignette focuses on MCMC diagnostic plots. Plots of parameter estimates from MCMC draws are covered in the separate vignette
[*Plotting MCMC draws using the bayesplot package*]("MCMC.html").

## General MCMC diagnostics

A Markov chain generates samples from the target distribution only after it has
converged to equilibrium. Unfortunately, this is only guaranteed in the limit
in theory. In practice, diagnostics must be applied to monitor whether the
Markov chain(s) have converged.

The __bayesplot__ package provides various plotting functions for visualizing 
Markov chain Monte Carlo (MCMC) diagnostics after fitting a Bayesian model. In
this vignette we'll use MCMC draws obtained using the `stan_glm` function in the
**rstanarm** package (Gabry and Goodrich, 2016), but MCMC draws from 
any package can be used with the functions in the **bayesplot** package. See, 
for example, **brms** (which, like **rstanarm**, calls the **rstan** package 
internally to use [Stan](http://mc-stan.org/)'s MCMC sampler).

### Rhat: potential scale reduction statistic

> One way to monitor whether a chain has converged to the equilibrium distribution
is to compare its behavior to other randomly initialized chains. This is the 
motivation for the Gelman and Rubin (1992) potential scale reduction statistic, 
$\hat{R}$. The $\hat{R}$ statistic measures the ratio of the average variance of
samples within each chain to the variance of the pooled samples across chains;
if all chains are at equilibrium, these will be the same and $\hat{R}$ will be
one. If the chains have not converged to a common distribution, the $\hat{R}$
statistic will be greater than one. ([Stan Modeling Language Users Guide and Reference Manual](http://mc-stan.org/documentation/))

The **bayesplot** package provides the functions `mcmc_rhat` and 
`mcmc_rhat_hist` for visualizing $\hat{R}$ estimates. To demostrate, we'll use
the `stan_glm` function in the **rstanarm** package (Gabry and Goodrich, 2016)
to quickly get some real $\hat{R}$ values to work with.

First we'll intentionally use too few MCMC iterations, which should 
lead to some high $\hat{R}$ values (we'll probably get convergence
warnings from **rstanarm**).

```{r, eval=FALSE, results='hide'}
library("rstanarm")
fit <- stan_glm(
  mpg ~ ., data = mtcars, 
  chains = 4, iter = 50, # intentionally too few iterations
  seed = 1111
)
```

```{r stan_glm, echo=FALSE, results='hide'}
suppressPackageStartupMessages(library("rstanarm"))
fit <- stan_glm(
  mpg ~ ., data = mtcars, 
  chains = 4, 
  iter = 50,
  seed = 1111
)
```

The **bayesplot** package provides a generic `rhat` extractor function, 
currently with methods defined for models fit using the **rstan** and
**rstanarm** packages. But regardless of how you fit your model, all 
**bayesplot** needs is a vector of $\hat{R}$ values.

```{r print-rhats}
library("bayesplot")
rhats <- rhat(fit)
print(rhats)
```

We can visualize the $\hat{R}$ values with the `mcmc_rhat` function:

```{r mcmc_rhat}
set_color_scheme("brightblue") # see ?set_color_scheme
mcmc_rhat(rhats)
```

In the plot, the points representing the $\hat{R}$ values are colored based on
whether they are less than $1.05$, between $1.05$ and $1.1$, or greater than
$1.1$.

We can see which parameters have the concerning $\hat{R}$ values by turning
on the $y$-axis text using the `yaxis_text` convenience function:

```{r, mcmc_rhat-2}
mcmc_rhat(rhats) + yaxis_text()
```

The axis $y$-axis text is off by default for this plot because it's only 
possible to see the labels for models like this one with very few parameters.

If we refit the model using longer Markov chains we should see all $\hat{R} <
1.1$, and all points in the plot the same (light) color:

```{r, results='hide'}
fit2 <- update(fit, iter = 1000)

mcmc_rhat(rhat = rhat(fit2))
```

We can see the same information shown by `mcmc_rhat` but in histogram form using
the `mcmc_rhat_hist` function. See the **Examples** section in
`help("mcmc_rhat_hist")` for examples.


### Effective sample size

The effective sample size is an estimate of the number of 
independent draws from the posterior distribution of the estimand of interest. 
Because the draws within a Markov chain are not independent if there is
autocorrelation, the effective sample size, $n_{eff}$, will be smaller than the total sample size, $N$. The larger the ratio of $n_{eff}$ to $N$ the better.

The **bayesplot** package provides the functions `mcmc_neff` and
`mcmc_neff_hist` for visualizing $n_{eff}/N$ estimates. For demonstration we'll use the models we fit in the $\hat{R}$ examples above.

The **bayesplot** package provides a generic `neff_ratio` extractor function, 
currently with methods defined for models fit using the **rstan** and
**rstanarm** packages. But regardless of how you fit your model, all 
**bayesplot** needs is a vector of $n_{eff}/N$ values.

```{r print-neff-ratios}
ratios <- neff_ratio(fit)
print(ratios)
```

We can visualize the these values with the `mcmc_neff` function:

```{r mcmc_neff}
mcmc_neff(ratios)
```

In the plot, the points representing the values of $n_{eff}/N$ are colored based
on whether they are less than $0.1$, between $0.1$ and $0.5$, or greater than 
$0.5$. These particular values are arbitrary in that they have no particular
theoretical meaning, but a useful heuristic is to worry about any $n_{eff}/N$
less than $0.1$.

If we look at the same plot for `fit2` we see a much nicer picture, with 
all ratios well above $0.1$:

```{r mcmc_neff-2}
mcmc_neff(neff_ratio(fit2))
```

One important thing to keep in mind is that these ratios will depend not only on
the model being fit but also on the particular MCMC algorithm used. One reason
why we have such high ratios of $n_{eff}$ to $N$ is that **rstanarm** uses
Hamiltonian Monte Carlo, which in general produces draws from the posterior
distribution with much lower autocorrelations compared to draws obtained using
other MCMC algorithms (e.g., Gibbs).


### Autocorrelation

As mentioned above, the effective sample size decreases as autocorrelation
becomes more extreme. We can visualize the autocorrelation using the 
`mcmc_acf` (line plot) or `mcmc_acf_bar` (bar plot) functions. Here are
plots showing the estimates of the autocorrelation function by chain for the 
`"(Intercept)"` parameter from `fit` (left) and `fit2` (right):

```{r mcmc_acf}
acf_fit <- mcmc_acf(as.array(fit), pars = "(Intercept)", lags = 15)
acf_fit2 <- mcmc_acf(as.array(fit2), pars = "(Intercept)", lags = 15)
gridExtra::grid.arrange(acf_fit, acf_fit2, ncol = 2)
```


## Diagnostics for the No-U-Turn Sampler

The No-U-Turn Sampler (NUTS, Hoffman and Gelman 2014) is the variant of
Hamiltonian Monte Carlo (HMC) used by [Stan](http://mc-stan.org/) and the various R
packages that depend on Stan for fitting Bayesian models. 
The **bayesplot** package has special functions for visualizing some of the unique diagnostics permitted by HMC, 
and NUTS in particular. The special **bayesplot** functions for 
NUTS diagnostics are 

* `mcmc_nuts_acceptance`
* `mcmc_nuts_divergence`
* `mcmc_nuts_stepsize`
* `mcmc_nuts_treedepth`
* `mcmc_nuts_energy`

The **bayesplot** package also provides generic functions `log_posterior` and
`nuts_params` for extracting the required information from fitted model objects.
Currently methods are provided for models fit using the **rstan** and
**rstanarm** packages, although it is not difficult to define additional methods
for the objects returned by other R packages.


In future versions of the **bayesplot** package this vignette will be expanded 
to provide examples of creating and interpreting the NUTS diagnostic plots. For 
now see `help("MCMC-nuts", package = "bayesplot")` and the references below.

## References

Betancourt, M. (2016). Diagnosing suboptimal cotangent disintegrations in
Hamiltonian Monte Carlo. https://arxiv.org/abs/1604.00695

Betancourt, M. and Girolami, M. (2013). Hamiltonian Monte Carlo for hierarchical
models. https://arxiv.org/abs/1312.0906

Gabry, J., and Goodrich, B. (2016). rstanarm: Bayesian Applied Regression 
Modeling via Stan. R package version 2.11.1. 
http://mc-stan.org/interfaces/rstanarm.html

Gelman, A. and Rubin, D. B. (1992). Inference from iterative simulation using
multiple sequences. *Statistical Science*. 7(4): 457--472.

Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin,
D. B. (2013). *Bayesian Data Analysis*. Chapman & Hall/CRC Press, London, third
edition.

Hoffman, M. D. and Gelman, A. (2014). The No-U-Turn Sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. *Journal of Machine Learning Research*. 15:1593--1623.

Stan Development Team. (2016). *Stan Modeling Language Users
Guide and Reference Manual*. http://mc-stan.org/documentation/

